{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries imported successfully!\n",
      "📅 Current date: 2025-07-22 13:03:28\n",
      "⏱️  Script execution started at: 13:03:28\n"
     ]
    }
   ],
   "source": [
    "# ===== STANDARD LIBRARY IMPORTS =====\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from urllib.parse import quote_plus, urlsplit, urlunsplit\n",
    "\n",
    "# ===== THIRD-PARTY DATA & ANALYSIS =====\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sympy import flatten\n",
    "\n",
    "# ===== WEB SCRAPING & HTTP =====\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ===== SELENIUM WEB AUTOMATION =====\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "# ===== DATABASE CONNECTIVITY =====\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# ===== CONCURRENCY & THREADING =====\n",
    "import concurrent.futures\n",
    "import queue\n",
    "\n",
    "# Get today's date and start timer\n",
    "current_date = datetime.today()\n",
    "script_start_time = time.time()\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(f\"📅 Current date: {current_date.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"⏱️  Script execution started at: {datetime.now().strftime('%H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL utility functions (imports now in cell 1)\n",
    "def remove_query_from_url(url):\n",
    "    url_parts = urlsplit(url)\n",
    "    url_without_query = urlunsplit((url_parts.scheme, url_parts.netloc, url_parts.path, '', ''))\n",
    "    return url_without_query\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stubhub_url(artist):\n",
    "    return \"https://www.stubhub.ca/secure/search?q=\" + artist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def click_checkboxes(driver):\n",
    "    \"\"\"\n",
    "    Clicks each checkbox, unchecks all others, and returns the link each time.\n",
    "    \"\"\"\n",
    "    # Find and click the \"Zones\" button to expand the filter options\n",
    "\n",
    "    zones =  driver.find_element(By.ID, \"stubhub-event-detail-ticket-class-filter\")\n",
    "    zones.click()\n",
    "    checkboxes = zones.find_elements(By.CSS_SELECTOR, 'input[type=\"checkbox\"]')\n",
    "    links = []\n",
    "\n",
    "    for checkbox in checkboxes:\n",
    "        # Uncheck all checkboxes first\n",
    "        for cb in checkboxes:\n",
    "            if cb.is_selected():\n",
    "                cb.click()\n",
    "        checkbox.click()\n",
    "        links.append(driver.current_url)\n",
    "    if len(links) == 0:\n",
    "        links.append(driver.current_url)\n",
    "    return links\n",
    "\n",
    "# Call the function and store the links\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../Documents/Ticket Sales.xlsx\"\n",
    "unique_artists = pd.read_excel(path, sheet_name =\"Events\")[\"Artist\"].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cocunrrency Executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ChunkedEncodingError",
     "evalue": "('Connection broken: IncompleteRead(3266 bytes read, 6974 more expected)', IncompleteRead(3266 bytes read, 6974 more expected))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIncompleteRead\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\response.py:754\u001b[39m, in \u001b[36mHTTPResponse._error_catcher\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    753\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m754\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    757\u001b[39m     \u001b[38;5;66;03m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;66;03m# there is yet no clean way to get at it from this context.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\response.py:1222\u001b[39m, in \u001b[36mHTTPResponse.read_chunked\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1221\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m chunk = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1223\u001b[39m decoded = \u001b[38;5;28mself\u001b[39m._decode(\n\u001b[32m   1224\u001b[39m     chunk, decode_content=decode_content, flush_decoder=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1225\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\response.py:1159\u001b[39m, in \u001b[36mHTTPResponse._handle_chunk\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m   1158\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt < \u001b[38;5;28mself\u001b[39m.chunk_left:\n\u001b[32m-> \u001b[39m\u001b[32m1159\u001b[39m     value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_safe_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1160\u001b[39m     \u001b[38;5;28mself\u001b[39m.chunk_left = \u001b[38;5;28mself\u001b[39m.chunk_left - amt\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py:640\u001b[39m, in \u001b[36mHTTPResponse._safe_read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    639\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) < amt:\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(data, amt-\u001b[38;5;28mlen\u001b[39m(data))\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[31mIncompleteRead\u001b[39m: IncompleteRead(3266 bytes read, 6974 more expected)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mProtocolError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    819\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\response.py:1063\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1062\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.supports_chunked_reads():\n\u001b[32m-> \u001b[39m\u001b[32m1063\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_chunked(amt, decode_content=decode_content)\n\u001b[32m   1064\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\response.py:1202\u001b[39m, in \u001b[36mHTTPResponse.read_chunked\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1197\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m BodyNotHttplibCompatible(\n\u001b[32m   1198\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mBody should be http.client.HTTPResponse like. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1199\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIt should have have an fp attribute which returns raw chunks.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1200\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1202\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_error_catcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Don't bother reading the body of a HEAD request.\u001b[39;49;00m\n\u001b[32m   1204\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_original_response\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_response_to_head\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_original_response\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\response.py:781\u001b[39m, in \u001b[36mHTTPResponse._error_catcher\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (HTTPException, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m781\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ProtocolError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConnection broken: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    783\u001b[39m \u001b[38;5;66;03m# If no exception is thrown, we should avoid cleaning up\u001b[39;00m\n\u001b[32m    784\u001b[39m \u001b[38;5;66;03m# unnecessarily.\u001b[39;00m\n",
      "\u001b[31mProtocolError\u001b[39m: ('Connection broken: IncompleteRead(3266 bytes read, 6974 more expected)', IncompleteRead(3266 bytes read, 6974 more expected))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mChunkedEncodingError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 104\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# unique_artists = ['black pink']\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m concurrent.futures.ThreadPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     events = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfetch_artist_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munique_artists\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m events = flatten(events)\n\u001b[32m    106\u001b[39m events = \u001b[38;5;28mlist\u001b[39m({v[\u001b[33m'\u001b[39m\u001b[33meventId\u001b[39m\u001b[33m'\u001b[39m]: v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m events \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}.values())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[32m    617\u001b[39m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m         fut.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\thread.py:58\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mfetch_artist_data\u001b[39m\u001b[34m(artist)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfetch_artist_data\u001b[39m(artist):\n\u001b[32m      9\u001b[39m     artist_search_url = generate_stubhub_url(artist)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist_search_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     soup = BeautifulSoup(response.content, \u001b[33m'\u001b[39m\u001b[33mhtml.parser\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     12\u001b[39m     event_grid = soup.find(\u001b[33m'\u001b[39m\u001b[33mscript\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m=\u001b[33m'\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m'\u001b[39m, string=\u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33meventGrids\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:746\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    743\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m     \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\models.py:902\u001b[39m, in \u001b[36mResponse.content\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    900\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    901\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m902\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mself\u001b[39m.iter_content(CONTENT_CHUNK_SIZE)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    904\u001b[39m \u001b[38;5;28mself\u001b[39m._content_consumed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\models.py:822\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    820\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    824\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ContentDecodingError(e)\n",
      "\u001b[31mChunkedEncodingError\u001b[39m: ('Connection broken: IncompleteRead(3266 bytes read, 6974 more expected)', IncompleteRead(3266 bytes read, 6974 more expected))"
     ]
    }
   ],
   "source": [
    "# Initialize variables\n",
    "event_listing = []\n",
    "driver_queue = queue.Queue()\n",
    "processed_titles = []\n",
    "short_cols = ['id', 'eventId', 'Artist', 'Venue', 'City', 'Event Date', 'countryName', 'Event Name', 'Event Type', 'Performer Type', 'Performer', 'section', 'sectionId', 'sectionMapName', 'sectionType', 'row', 'rowId', 'seat', 'seatFrom', 'seatTo', 'faceValue', 'rawPrice', 'priceWithFees', 'price', 'ticketClass', 'ticketClassName', 'ticketTypeId', 'ticketTypeGroupId', 'listingTypeId', 'listingCurrencyCode', 'buyerCurrencyCode', 'faceValueCurrencyCode', 'Updated', 'formattedDealScore']\n",
    "\n",
    "# Function to fetch artist data\n",
    "def fetch_artist_data(artist):\n",
    "    artist_search_url = generate_stubhub_url(artist)\n",
    "    response = requests.get(artist_search_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    event_grid = soup.find('script', type='application/json', string=lambda x: x and 'eventGrids' in x)\n",
    "    if event_grid is None:\n",
    "        return None\n",
    "    json_str = event_grid.text.strip()\n",
    "    data = json.loads(json_str)\n",
    "    event_grid = data[\"eventGrids\"]\n",
    "    events = event_grid[\"0\"][\"items\"][0:7]\n",
    "    for event in events:\n",
    "        event[\"Artist\"] = artist\n",
    "    return events\n",
    "\n",
    "# Function to get listing info\n",
    "def get_listing_info(url):\n",
    "    response = requests.get(url)\n",
    "    page_source = response.text\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    script_tag = soup.find('script', id='index-data', type='application/json')\n",
    "    if script_tag is None:\n",
    "        return None\n",
    "    json_string = script_tag.string\n",
    "    index_data = json.loads(json_string)\n",
    "    grid_items = index_data['grid']['items']\n",
    "    df = pd.DataFrame(grid_items)\n",
    "    formatted_date = datetime.strptime(index_data[\"formattedEventDateTime\"], '%a %b %d %Y %I:%M %p').date()\n",
    "    df[\"Event Date\"] = formatted_date\n",
    "    df[\"Updated\"] = pd.Timestamp.today().strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "    script_tag = soup.find('script', type='application/ld+json')\n",
    "    if script_tag:\n",
    "        event_data = json.loads(script_tag.string)\n",
    "        df[\"Event Type\"] = event_data[\"@type\"]\n",
    "        df[\"Performer Type\"] = event_data[\"performer\"][0][\"@type\"]\n",
    "        df[\"Performer\"] = event_data[\"performer\"][0][\"name\"]\n",
    "    \n",
    "    df = df[df.columns.intersection(short_cols)]\n",
    "    return df\n",
    "\n",
    "# Function to attach event data\n",
    "def attach_event_data(df, event):\n",
    "    df[\"Venue\"] = event[\"venueName\"]\n",
    "    df[\"Artist\"] = event[\"Artist\"]\n",
    "    df[\"Event Name\"] = event[\"name\"]\n",
    "    df[\"City\"] = event[\"venueCity\"]\n",
    "    df[\"countryName\"] = event['countryName']\n",
    "    return df\n",
    "\n",
    "# Function to get listing dataframe\n",
    "def get_listing_df(checkbox_links, event):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        listings = list(executor.map(get_listing_info, checkbox_links))\n",
    "    curr = pd.concat(listings, ignore_index=True)\n",
    "    curr = attach_event_data(curr, event)\n",
    "    event_listing.append(curr)\n",
    "\n",
    "# Function to get checkbox links\n",
    "def get_checkbox_links(event, listing_executor):\n",
    "    driver = driver_queue.get(block=True)\n",
    "\n",
    "    try:\n",
    "        url = event[\"url\"]\n",
    "        url = remove_query_from_url(url) + \"?listingQty=&quantity=0\"+\"&betterValueTickets=false\" + \"&estimatedFees=false\"\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH, '//button[text()=\"Continue\"]'))).click()\n",
    "        except:\n",
    "            pass\n",
    "        WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH, '//div[@aria-label=\"Filters\"]'))).click() # open filter menu\n",
    "        checkbox_links = click_checkboxes(driver)\n",
    "    except:\n",
    "        checkbox_links = [url]\n",
    "    listing_executor.submit(get_listing_df, checkbox_links, event)\n",
    "    driver_queue.put(driver)\n",
    "\n",
    "# Function to create driver\n",
    "def create_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--no-sandbox')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver_queue.put(driver)\n",
    "# Close drivers\n",
    "def close_driver(driver):\n",
    "    driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_artists = ['black pink']\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    events = list(executor.map(fetch_artist_data, unique_artists))\n",
    "events = flatten(events)\n",
    "events = list({v['eventId']: v for v in events if v is not None}.values())\n",
    "events = [event for event in events if event['countryName'] in ['Canada', 'USA']]\n",
    "\n",
    "# Get checkbox links\n",
    "with concurrent.futures.ThreadPoolExecutor(10) as selenium_executor, concurrent.futures.ThreadPoolExecutor() as listing_executor, concurrent.futures.ThreadPoolExecutor() as driver_executor:\n",
    "    selenium_executor.map(lambda _: create_driver(), range(8))\n",
    "    selenium_executor.map(lambda x: get_checkbox_links(x, listing_executor), events)\n",
    "    selenium_executor.shutdown(wait=True)\n",
    "    driver_executor.map(close_driver, list(driver_queue.queue))\n",
    "    listing_executor.shutdown(wait=True)\n",
    "\n",
    "combined_df = pd.concat(event_listing, ignore_index=True)\n",
    "combined_df.drop_duplicates(subset=['id', 'eventId'], keep='last', inplace=True)\n",
    "\n",
    "combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning price columns...\n",
      "✓ Cleaned price\n",
      "✓ Cleaned faceValue\n",
      "\n",
      "Sample cleaned priceWithFees values:\n"
     ]
    }
   ],
   "source": [
    "# Price cleaning functions (imports now in cell 1)\n",
    "def clean_price_value(value):\n",
    "    \"\"\"\n",
    "    Clean price values by removing currency symbols and converting to float.\n",
    "    \"\"\"\n",
    "    if pd.isna(value) or value is None:\n",
    "        return None\n",
    "    str_value = str(value)\n",
    "    cleaned = re.sub(r'[C$€£¥,\\s]', '', str_value)\n",
    "    try:\n",
    "        return float(cleaned) if cleaned else None\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "print(\"Cleaning price columns...\")\n",
    "\n",
    "\n",
    "\n",
    "if 'price' in combined_df.columns:\n",
    "    combined_df['price'] = combined_df['price'].apply(clean_price_value)\n",
    "    print(\"✓ Cleaned price\")\n",
    "\n",
    "if 'faceValue' in combined_df.columns:\n",
    "    combined_df['faceValue'] = combined_df['faceValue'].apply(clean_price_value)\n",
    "    print(\"✓ Cleaned faceValue\")\n",
    "\n",
    "print(f\"\\nSample cleaned priceWithFees values:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.rename(columns={'Artist Name': 'Artist'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['Artist'] = combined_df['Artist'].astype(str).str.title()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Integration\n",
    "\n",
    "### Insert Scraped Data into CONCERT_SEATS Table\n",
    "\n",
    "Now we'll insert the scraped concert seat data from `combined_df` into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Database configuration loaded:\n",
      "Host: 192.168.68.74\n",
      "User: root\n",
      "Database: concert\n",
      "Password: [HIDDEN]\n"
     ]
    }
   ],
   "source": [
    "# Load database configuration (imports now in cell 1)\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Load database configuration\n",
    "try:\n",
    "    with open('db.json', 'r') as f:\n",
    "        db_config = json.load(f)\n",
    "    \n",
    "    print(\"\\nDatabase configuration loaded:\")\n",
    "    print(f\"Host: {db_config['host']}\")\n",
    "    print(f\"User: {db_config['user']}\")\n",
    "    print(f\"Database: {db_config['database']}\")\n",
    "    print(\"Password: [HIDDEN]\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ db.json file not found. Please ensure it exists in the current directory.\")\n",
    "    db_config = None\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading database configuration: {e}\")\n",
    "    db_config = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to insert 36057 rows into CONCERT_SEATS table...\n",
      "Combined DataFrame columns: ['id', 'eventId', 'section', 'sectionId', 'sectionMapName', 'sectionType', 'row', 'seat', 'ticketClass', 'ticketClassName', 'rowId', 'rawPrice', 'price', 'ticketTypeId', 'ticketTypeGroupId', 'listingTypeId', 'listingCurrencyCode', 'buyerCurrencyCode', 'faceValue', 'faceValueCurrencyCode', 'formattedDealScore', 'Event Date', 'Updated', 'Event Type', 'Performer Type', 'Performer', 'Venue', 'Artist', 'Event Name', 'City', 'countryName', 'seatFrom', 'seatTo']\n",
      "Added missing column: priceWithFees\n",
      "Added missing column: isFavorite\n",
      "Added missing column: aggregateFavorites\n",
      "Added missing column: isStanding\n",
      "Added missing column: formattedFees\n",
      "✓ Data preparation completed\n",
      "✓ Data preparation completed\n"
     ]
    }
   ],
   "source": [
    "# Prepare the combined_df for database insertion\n",
    "print(f\"Preparing to insert {len(combined_df)} rows into CONCERT_SEATS table...\")\n",
    "print(f\"Combined DataFrame columns: {list(combined_df.columns)}\")\n",
    "\n",
    "# Ensure all required columns exist (add missing ones as None)\n",
    "required_columns = [\n",
    "    'id', 'section', 'sectionId', 'row', 'rowId', 'faceValue', 'rawPrice', 'priceWithFees',\n",
    "    'eventId', 'sectionMapName', 'sectionType', 'seat', 'seatFrom', 'seatTo', 'ticketClass',\n",
    "    'ticketClassName', 'price', 'ticketTypeId', 'ticketTypeGroupId', 'listingTypeId', 'City',\n",
    "    'Event Date', 'countryName', 'Event Name', 'Event Type', 'Performer Type', 'Performer',\n",
    "    'isFavorite', 'aggregateFavorites', 'listingCurrencyCode', 'buyerCurrencyCode',\n",
    "    'faceValueCurrencyCode', 'Updated', 'isStanding', 'formattedFees', 'Artist', 'Venue',\n",
    "    'formattedDealScore'\n",
    "]\n",
    "\n",
    "# Add missing columns with None values\n",
    "for col in required_columns:\n",
    "    if col not in combined_df.columns:\n",
    "        combined_df[col] = None\n",
    "        print(f\"Added missing column: {col}\")\n",
    "\n",
    "# Rename 'row' to 'row_name' to match database schema\n",
    "if 'row' in combined_df.columns:\n",
    "    combined_df.rename(columns={'row': 'row_name'}, inplace=True)\n",
    "\n",
    "# Ensure date columns are properly formatted for MySQL\n",
    "if 'Event Date' in combined_df.columns:\n",
    "    # Convert to datetime first, then to string format for MySQL\n",
    "    combined_df['Event Date'] = pd.to_datetime(combined_df['Event Date'], errors='coerce')\n",
    "    combined_df['Event Date'] = combined_df['Event Date'].dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "if 'Updated' in combined_df.columns:\n",
    "    # Convert to datetime first, then to string format for MySQL  \n",
    "    combined_df['Updated'] = pd.to_datetime(combined_df['Updated'], errors='coerce')\n",
    "    combined_df['Updated'] = combined_df['Updated'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "print(\"✓ Data preparation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Cleaning data for MySQL compatibility...\n",
      "✓ Data cleaned. Shape: (36057, 38)\n",
      "✓ sectionType converted to numeric: int64\n",
      "✓ Data cleaned. Shape: (36057, 38)\n",
      "✓ sectionType converted to numeric: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eric9\\AppData\\Local\\Temp\\ipykernel_14472\\1842555679.py:103: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_for_insert[col] = df_for_insert[col].fillna(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Performing UPSERT operation...\n",
      "  Processed 1000/36057 rows...\n",
      "  Processed 1000/36057 rows...\n",
      "  Processed 2000/36057 rows...\n",
      "  Processed 2000/36057 rows...\n",
      "  Processed 3000/36057 rows...\n",
      "  Processed 3000/36057 rows...\n",
      "  Processed 4000/36057 rows...\n",
      "  Processed 4000/36057 rows...\n",
      "  Processed 5000/36057 rows...\n",
      "  Processed 5000/36057 rows...\n",
      "  Processed 6000/36057 rows...\n",
      "  Processed 6000/36057 rows...\n",
      "  Processed 7000/36057 rows...\n",
      "  Processed 7000/36057 rows...\n",
      "  Processed 8000/36057 rows...\n",
      "  Processed 8000/36057 rows...\n",
      "  Processed 9000/36057 rows...\n",
      "  Processed 9000/36057 rows...\n",
      "  Processed 10000/36057 rows...\n",
      "  Processed 10000/36057 rows...\n",
      "  Processed 11000/36057 rows...\n",
      "  Processed 11000/36057 rows...\n",
      "  Processed 12000/36057 rows...\n",
      "  Processed 12000/36057 rows...\n",
      "  Processed 13000/36057 rows...\n",
      "  Processed 13000/36057 rows...\n",
      "  Processed 14000/36057 rows...\n",
      "  Processed 14000/36057 rows...\n",
      "  Processed 15000/36057 rows...\n",
      "  Processed 15000/36057 rows...\n",
      "  Processed 16000/36057 rows...\n",
      "  Processed 16000/36057 rows...\n",
      "  Processed 17000/36057 rows...\n",
      "  Processed 17000/36057 rows...\n",
      "  Processed 18000/36057 rows...\n",
      "  Processed 18000/36057 rows...\n",
      "  Processed 19000/36057 rows...\n",
      "  Processed 19000/36057 rows...\n",
      "  Processed 20000/36057 rows...\n",
      "  Processed 20000/36057 rows...\n",
      "  Processed 21000/36057 rows...\n",
      "  Processed 21000/36057 rows...\n",
      "  Processed 22000/36057 rows...\n",
      "  Processed 22000/36057 rows...\n",
      "  Processed 23000/36057 rows...\n",
      "  Processed 23000/36057 rows...\n",
      "  Processed 24000/36057 rows...\n",
      "  Processed 24000/36057 rows...\n",
      "  Processed 25000/36057 rows...\n",
      "  Processed 25000/36057 rows...\n",
      "  Processed 26000/36057 rows...\n",
      "  Processed 26000/36057 rows...\n",
      "  Processed 27000/36057 rows...\n",
      "  Processed 27000/36057 rows...\n",
      "  Processed 28000/36057 rows...\n",
      "  Processed 28000/36057 rows...\n",
      "  Processed 29000/36057 rows...\n",
      "  Processed 29000/36057 rows...\n",
      "  Processed 30000/36057 rows...\n",
      "  Processed 30000/36057 rows...\n",
      "  Processed 31000/36057 rows...\n",
      "  Processed 31000/36057 rows...\n",
      "  Processed 32000/36057 rows...\n",
      "  Processed 32000/36057 rows...\n",
      "  Processed 33000/36057 rows...\n",
      "  Processed 33000/36057 rows...\n",
      "  Processed 34000/36057 rows...\n",
      "  Processed 34000/36057 rows...\n",
      "  Processed 35000/36057 rows...\n",
      "  Processed 35000/36057 rows...\n",
      "  Processed 36000/36057 rows...\n",
      "  Processed 36000/36057 rows...\n",
      "  Processed 36057/36057 rows...\n",
      "  Processed 36057/36057 rows...\n",
      "✅ Successfully inserted/updated 36057 rows into CONCERT_SEATS table!\n",
      "   Batch size: 1000\n",
      "   Database: concert\n",
      "Database connection closed.\n",
      "✅ Successfully inserted/updated 36057 rows into CONCERT_SEATS table!\n",
      "   Batch size: 1000\n",
      "   Database: concert\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "# Insert data into CONCERT_SEATS table using raw SQL (most reliable method)\n",
    "try:\n",
    "    # Build SQLAlchemy connection string\n",
    "    user = db_config['user']\n",
    "    password = db_config['password']\n",
    "    host = db_config['host']\n",
    "    database = db_config['database']\n",
    "    \n",
    "    # URL-encode the password to handle special characters\n",
    "    encoded_password = quote_plus(password)\n",
    "    connection_string = f\"mysql+mysqlconnector://{user}:{encoded_password}@{host}/{database}\"\n",
    "    \n",
    "    # Create SQLAlchemy engine\n",
    "    engine = create_engine(connection_string)\n",
    "    \n",
    "    # Prepare final column mapping to match database schema\n",
    "    db_columns = {\n",
    "        'id': 'id',\n",
    "        'section': 'section', \n",
    "        'sectionId': 'sectionId',\n",
    "        'row_name': 'row_name',\n",
    "        'rowId': 'rowId',\n",
    "        'faceValue': 'faceValue',\n",
    "        'rawPrice': 'rawPrice',\n",
    "        'priceWithFees': 'priceWithFees',\n",
    "        'eventId': 'eventId',\n",
    "        'sectionMapName': 'sectionMapName',\n",
    "        'sectionType': 'sectionType',\n",
    "        'seat': 'seat',\n",
    "        'seatFrom': 'seatFrom', \n",
    "        'seatTo': 'seatTo',\n",
    "        'ticketClass': 'ticketClass',\n",
    "        'ticketClassName': 'ticketClassName',\n",
    "        'price': 'price',\n",
    "        'ticketTypeId': 'ticketTypeId',\n",
    "        'ticketTypeGroupId': 'ticketTypeGroupId',\n",
    "        'listingTypeId': 'listingTypeId',\n",
    "        'City': 'city',\n",
    "        'Event Date': 'event_date',\n",
    "        'countryName': 'countryName',\n",
    "        'Event Name': 'event_name',\n",
    "        'Event Type': 'event_type',\n",
    "        'Performer Type': 'performer_type',\n",
    "        'Performer': 'performer',\n",
    "        'isFavorite': 'isFavorite',\n",
    "        'aggregateFavorites': 'aggregateFavorites',\n",
    "        'listingCurrencyCode': 'listingCurrencyCode',\n",
    "        'buyerCurrencyCode': 'buyerCurrencyCode',\n",
    "        'faceValueCurrencyCode': 'faceValueCurrencyCode',\n",
    "        'Updated': 'updated_date',\n",
    "        'isStanding': 'isStanding',\n",
    "        'formattedFees': 'formattedFees',\n",
    "        'Artist': 'artist',\n",
    "        'Venue': 'venue',\n",
    "        'formattedDealScore': 'formattedDealScore'\n",
    "    }\n",
    "    \n",
    "    # Select and rename columns to match database schema\n",
    "    df_for_insert = combined_df.rename(columns=db_columns)\n",
    "    \n",
    "    # Clean the data for MySQL compatibility\n",
    "    print(\"🧹 Cleaning data for MySQL compatibility...\")\n",
    "    \n",
    "    # Replace NaN/None values with appropriate defaults for string columns\n",
    "    df_for_insert = df_for_insert.fillna({\n",
    "        'section': '',\n",
    "        'sectionMapName': '',\n",
    "        'seat': '',\n",
    "        'row_name': '',\n",
    "        'ticketClass': '',\n",
    "        'ticketClassName': '',\n",
    "        'listingCurrencyCode': 'CAD',\n",
    "        'buyerCurrencyCode': 'CAD', \n",
    "        'faceValueCurrencyCode': 'CAD',\n",
    "        'city': '',\n",
    "        'countryName': '',\n",
    "        'event_name': '',\n",
    "        'event_type': '',\n",
    "        'performer_type': '',\n",
    "        'performer': '',\n",
    "        'artist': '',\n",
    "        'venue': '',\n",
    "        'formattedFees': '',\n",
    "        'formattedDealScore': ''\n",
    "    })\n",
    "    \n",
    "    # Fill numeric columns with 0 or appropriate defaults\n",
    "    numeric_cols = ['faceValue', 'rawPrice', 'priceWithFees', 'price', 'seatFrom', 'seatTo']\n",
    "    for col in numeric_cols:\n",
    "        if col in df_for_insert.columns:\n",
    "            df_for_insert[col] = pd.to_numeric(df_for_insert[col], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Fill integer columns with 0 - INCLUDING sectionType which should be numeric\n",
    "    int_cols = ['sectionId', 'rowId', 'ticketTypeId', 'ticketTypeGroupId', 'listingTypeId', 'eventId', 'sectionType']\n",
    "    for col in int_cols:\n",
    "        if col in df_for_insert.columns:\n",
    "            df_for_insert[col] = pd.to_numeric(df_for_insert[col], errors='coerce').fillna(0).astype(int)\n",
    "    \n",
    "    # Fill boolean columns with False\n",
    "    bool_cols = ['isFavorite', 'isStanding']\n",
    "    for col in bool_cols:\n",
    "        if col in df_for_insert.columns:\n",
    "            df_for_insert[col] = df_for_insert[col].fillna(False)\n",
    "    \n",
    "    print(f\"✓ Data cleaned. Shape: {df_for_insert.shape}\")\n",
    "    print(f\"✓ sectionType converted to numeric: {df_for_insert['sectionType'].dtype}\")\n",
    "    \n",
    "    # Use raw SQL INSERT with ON DUPLICATE KEY UPDATE\n",
    "    batch_size = 1000\n",
    "    \n",
    "    # Get connection from engine\n",
    "    connection_sql = engine.raw_connection()\n",
    "    cursor_sql = connection_sql.cursor()\n",
    "    \n",
    "    try:\n",
    "        # Get column names for the INSERT statement\n",
    "        columns = list(df_for_insert.columns)\n",
    "        placeholders = ', '.join(['%s'] * len(columns))\n",
    "        columns_str = ', '.join([f'`{col}`' for col in columns])\n",
    "        \n",
    "        # Create UPDATE part (all columns except id)\n",
    "        update_columns = [col for col in columns if col != 'id']\n",
    "        update_str = ', '.join([f'`{col}` = VALUES(`{col}`)' for col in update_columns])\n",
    "        \n",
    "        # Build the UPSERT query\n",
    "        upsert_query = f\"\"\"\n",
    "        INSERT INTO CONCERT_SEATS ({columns_str})\n",
    "        VALUES ({placeholders})\n",
    "        ON DUPLICATE KEY UPDATE {update_str}\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"🔄 Performing UPSERT operation...\")\n",
    "        \n",
    "        # Execute batch insert with upsert logic\n",
    "        total_rows = len(df_for_insert)\n",
    "        rows_processed = 0\n",
    "        \n",
    "        for i in range(0, total_rows, batch_size):\n",
    "            batch_df = df_for_insert.iloc[i:i + batch_size]\n",
    "            batch_data = [tuple(row) for row in batch_df.values]\n",
    "            \n",
    "            cursor_sql.executemany(upsert_query, batch_data)\n",
    "            rows_processed += len(batch_data)\n",
    "            print(f\"  Processed {rows_processed}/{total_rows} rows...\")\n",
    "        \n",
    "        connection_sql.commit()\n",
    "        print(f\"✅ Successfully inserted/updated {len(df_for_insert)} rows into CONCERT_SEATS table!\")\n",
    "        print(f\"   Batch size: {batch_size}\")\n",
    "        print(f\"   Database: {database}\")\n",
    "        \n",
    "    finally:\n",
    "        cursor_sql.close()\n",
    "        connection_sql.close()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error inserting data into database: {e}\")\n",
    "    print(f\"   Make sure the CONCERT_SEATS table exists and has the correct schema.\")\n",
    "    \n",
    "finally:\n",
    "    # Close engine connection\n",
    "    if 'engine' in locals():\n",
    "        engine.dispose()\n",
    "        print(\"Database connection closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
